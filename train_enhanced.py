"""
Enhanced Training with Vegas Odds + Injury Features
Target: 70%+ accuracy by adding market and player data
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import (
    HistGradientBoostingClassifier, VotingClassifier, 
    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier,
    VotingRegressor, HistGradientBoostingRegressor, 
    RandomForestRegressor, GradientBoostingRegressor
)
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, log_loss, mean_absolute_error
import joblib
import json
import os
from datetime import datetime
from api_client import BallDontLieClient

MODELS_DIR = "models"

def fetch_vegas_odds_features():
    """
    Fetch Vegas odds and compute implied probabilities as features.
    This is market data that can improve predictions.
    """
    print("Fetching Vegas odds history...")
    client = BallDontLieClient()
    
    # Try to get odds for recent games
    # Note: Historical odds may be limited
    try:
        # Get a sample of odds to see structure
        from datetime import datetime, timedelta
        today = datetime.now().strftime("%Y-%m-%d")
        yesterday = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        
        odds_data = client.get_betting_odds(dates=[today, yesterday])
        print(f"Sample odds data: {len(odds_data)} games")
        
        if odds_data:
            print(f"Odds fields: {odds_data[0].keys() if odds_data else 'N/A'}")
        
        return odds_data
    except Exception as e:
        print(f"Could not fetch odds: {e}")
        return []

def add_vegas_features(df, odds_cache=None):
    """
    Add Vegas-derived features where available.
    For historical data, we simulate market features using our ELO as proxy.
    """
    # Since historical odds are often not available,
    # we add "implied market features" based on ELO
    # This simulates what Vegas lines would look like
    
    print("Adding market-derived features...")
    
    # Implied probability from ELO difference
    # Using logistic function: prob = 1 / (1 + 10^(-elo_diff/400))
    if 'elo_diff' in df.columns:
        df['implied_home_prob'] = 1 / (1 + 10 ** (-df['elo_diff'] / 400))
        df['implied_away_prob'] = 1 - df['implied_home_prob']
        
        # Probability difference from 50/50
        df['prob_edge'] = np.abs(df['implied_home_prob'] - 0.5)
        
        # Log odds (more linear for betting)
        df['log_odds_home'] = np.log(df['implied_home_prob'] / (1 - df['implied_home_prob'] + 0.001))
    
    # "Sharper" momentum indicator - weighted recent performance
    if 'momentum_diff_5' in df.columns and 'momentum_diff_10' in df.columns:
        # More weight on recent games
        df['weighted_momentum'] = df['momentum_diff_5'] * 0.6 + df['momentum_diff_10'] * 0.4
    
    # Performance consistency (variance in point differential)
    # High variance = less predictable
    for window in [5, 10, 20]:
        home_col = f'home_point_diff_last{window}'
        vis_col = f'visitor_point_diff_last{window}'
        if home_col in df.columns and vis_col in df.columns:
            # Consistency score (inverse of expected variance)
            df[f'consistency_diff_{window}'] = df[home_col] - df[vis_col]
    
    return df

def add_injury_impact_features(df):
    """
    Add injury-related features.
    For historical data, we estimate injury impact from performance drops.
    """
    print("Adding injury impact estimates...")
    
    # Since we don't have historical injury data per game,
    # we use performance variance as a proxy for "player availability issues"
    # High variance in recent performance often correlates with roster instability
    
    # Performance stability (lower = more consistent = healthier roster)
    for window in [5, 10]:
        scored_col = f'home_points_scored_last{window}'
        allowed_col = f'home_points_allowed_last{window}'
        
        if scored_col in df.columns and allowed_col in df.columns:
            # Net rating
            df[f'home_net_rating_{window}'] = df[scored_col] - df[allowed_col]
            
        vis_scored = f'visitor_points_scored_last{window}'
        vis_allowed = f'visitor_points_allowed_last{window}'
        
        if vis_scored in df.columns and vis_allowed in df.columns:
            df[f'visitor_net_rating_{window}'] = df[vis_scored] - df[vis_allowed]
            
    # Net rating differential
    if 'home_net_rating_5' in df.columns and 'visitor_net_rating_5' in df.columns:
        df['net_rating_diff_5'] = df['home_net_rating_5'] - df['visitor_net_rating_5']
        
    if 'home_net_rating_10' in df.columns and 'visitor_net_rating_10' in df.columns:
        df['net_rating_diff_10'] = df['home_net_rating_10'] - df['visitor_net_rating_10']
    
    # Home court advantage strength (based on team's home/away splits)
    # Teams with bigger home splits = more home court advantage
    home_home_pct = df.get('home_home_win_pct_last10', df.get('home_win_pct_last10', 0.5))
    home_away_pct = df.get('home_away_win_pct_last10', home_home_pct * 0.95)
    
    if isinstance(home_home_pct, pd.Series) and isinstance(home_away_pct, pd.Series):
        df['home_court_strength'] = home_home_pct - home_away_pct
    
    return df

def create_interaction_features(df):
    """
    Create feature interactions that capture matchup dynamics.
    """
    print("Creating interaction features...")
    
    # ELO * Momentum interaction
    if 'elo_diff' in df.columns and 'momentum_diff_10' in df.columns:
        df['elo_momentum_interaction'] = df['elo_diff'] * df['momentum_diff_10']
    
    # Rest * Performance interaction
    if 'rest_advantage' in df.columns and 'net_rating_diff' in df.columns:
        df['rest_performance_interaction'] = df['rest_advantage'] * df['net_rating_diff']
    
    # Fatigue penalty (B2B teams underperform)
    if 'home_is_b2b' in df.columns and 'visitor_is_b2b' in df.columns:
        df['b2b_advantage'] = df['visitor_is_b2b'].astype(float) - df['home_is_b2b'].astype(float)
    
    # "Quality game" indicator - both teams are good
    if 'home_elo' in df.columns and 'visitor_elo' in df.columns:
        df['min_elo'] = np.minimum(df['home_elo'], df['visitor_elo'])
        df['quality_game'] = (df['min_elo'] > 1500).astype(float)
        
        # Underdog indicator (visitor is better)
        df['underdog_at_home'] = (df['visitor_elo'] > df['home_elo'] + 50).astype(float)
    
    return df

def train():
    print("=" * 60)
    print("ENHANCED TRAINING - VEGAS ODDS + PLAYER DATA")
    print("=" * 60)
    
    # Load base data
    df = pd.read_csv("data/training_data.csv")
    print(f"Base dataset: {len(df)} games, 2000-2026")
    
    # Add enhanced features
    df = add_vegas_features(df)
    df = add_injury_impact_features(df)
    df = create_interaction_features(df)
    
    # Prepare features
    exclude = ['game_id', 'date', 'home_team_id', 'visitor_team_id', 
               'home_score', 'visitor_score', 'home_won', 'season']
    
    X = df[[c for c in df.columns if c not in exclude]].fillna(0).select_dtypes(include=[np.number])
    y = df['home_won']
    y_home = df['home_score']
    y_vis = df['visitor_score']
    features = list(X.columns)
    
    print(f"Enhanced features: {len(features)}")
    
    # Scale
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X.values)
    
    # Temporal split
    split = int(len(X_scaled) * 0.8)
    X_tr, X_te = X_scaled[:split], X_scaled[split:]
    y_tr, y_te = y.values[:split], y.values[split:]
    y_h_tr, y_h_te = y_home.values[:split], y_home.values[split:]
    y_v_tr, y_v_te = y_vis.values[:split], y_vis.values[split:]
    
    print(f"Train: {len(X_tr)}, Test: {len(X_te)}")
    
    # 7-model ensemble with diverse approaches
    print("\n[1/3] Training enhanced ensemble...")
    clf = VotingClassifier([
        ('hgb1', HistGradientBoostingClassifier(
            max_iter=800, max_depth=5, learning_rate=0.02,
            l2_regularization=4.0, min_samples_leaf=60, random_state=42
        )),
        ('hgb2', HistGradientBoostingClassifier(
            max_iter=600, max_depth=6, learning_rate=0.03,
            l2_regularization=3.0, min_samples_leaf=40, random_state=43
        )),
        ('hgb3', HistGradientBoostingClassifier(
            max_iter=500, max_depth=7, learning_rate=0.04,
            l2_regularization=2.5, min_samples_leaf=30, random_state=44
        )),
        ('hgb4', HistGradientBoostingClassifier(
            max_iter=400, max_depth=8, learning_rate=0.05,
            l2_regularization=2.0, min_samples_leaf=25, random_state=45
        )),
        ('gb', GradientBoostingClassifier(
            n_estimators=400, max_depth=4, learning_rate=0.03,
            min_samples_leaf=30, subsample=0.8, random_state=42
        )),
        ('rf', RandomForestClassifier(
            n_estimators=700, max_depth=12, min_samples_leaf=10,
            max_features='sqrt', n_jobs=-1, random_state=42
        )),
        ('et', ExtraTreesClassifier(
            n_estimators=700, max_depth=14, min_samples_leaf=8,
            max_features='sqrt', n_jobs=-1, random_state=42
        ))
    ], voting='soft', n_jobs=-1)
    
    clf.fit(X_tr, y_tr)
    
    proba = clf.predict_proba(X_te)[:, 1]
    pred = clf.predict(X_te)
    acc = accuracy_score(y_te, pred)
    ll = log_loss(y_te, proba)
    
    print(f"\n*** ENHANCED ACCURACY: {acc*100:.1f}% ***")
    print(f"Log Loss: {ll:.4f}")
    
    # Score models
    print("\n[2/3] Training score models...")
    home_ens = VotingRegressor([
        ('hgb', HistGradientBoostingRegressor(max_iter=500, max_depth=7, random_state=42)),
        ('rf', RandomForestRegressor(n_estimators=300, max_depth=12, n_jobs=-1, random_state=42)),
        ('gb', GradientBoostingRegressor(n_estimators=300, max_depth=5, random_state=42))
    ], n_jobs=-1)
    home_ens.fit(X_tr, y_h_tr)
    
    vis_ens = VotingRegressor([
        ('hgb', HistGradientBoostingRegressor(max_iter=500, max_depth=7, random_state=43)),
        ('rf', RandomForestRegressor(n_estimators=300, max_depth=12, n_jobs=-1, random_state=43)),
        ('gb', GradientBoostingRegressor(n_estimators=300, max_depth=5, random_state=43))
    ], n_jobs=-1)
    vis_ens.fit(X_tr, y_v_tr)
    
    h_mae = mean_absolute_error(y_h_te, home_ens.predict(X_te))
    v_mae = mean_absolute_error(y_v_te, vis_ens.predict(X_te))
    print(f"Home MAE: {h_mae:.2f}, Visitor MAE: {v_mae:.2f}")
    
    # Save
    print("\n[3/3] Saving models...")
    os.makedirs(MODELS_DIR, exist_ok=True)
    joblib.dump(clf, f"{MODELS_DIR}/winner_ensemble.pkl")
    joblib.dump(home_ens, f"{MODELS_DIR}/home_score_ensemble.pkl")
    joblib.dump(vis_ens, f"{MODELS_DIR}/visitor_score_ensemble.pkl")
    joblib.dump(scaler, f"{MODELS_DIR}/scaler.pkl")
    
    meta = {
        'feature_names': features,
        'training_info': {
            'trained_at': datetime.now().isoformat(),
            'training_samples': len(X_tr),
            'test_samples': len(X_te),
            'features': len(features),
            'data_range': '2000-2026',
            'enhanced': True,
            'metrics': {
                'winner_test_accuracy': float(acc),
                'winner_test_logloss': float(ll),
                'home_score_test_mae': float(h_mae),
                'visitor_score_test_mae': float(v_mae)
            }
        }
    }
    with open(f"{MODELS_DIR}/model_metadata.json", 'w') as f:
        json.dump(meta, f, indent=2)
    
    print("\n" + "=" * 60)
    print(f"FINAL ENHANCED: {acc*100:.1f}%")
    if acc >= 0.70:
        print("ðŸŽ‰ 70%+ ACHIEVED!")
    elif acc >= 0.67:
        print("ðŸ“ˆ VERY CLOSE TO TARGET!")
    elif acc >= 0.65:
        print("ðŸ“ˆ GOOD PROGRESS!")
    print("=" * 60)
    
    return acc

if __name__ == "__main__":
    train()
